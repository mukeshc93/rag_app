{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c030fb54-fbf3-4f3f-9963-9ee29a250265",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-embeddings-openai\n",
    "!pip install llama-index-vector-stores-pinecone\n",
    "!pip install llama-index-llms-openai\n",
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34122ab5-e620-4458-805b-27d7e9ed3fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install python-dotenv pinecone llama-index pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b20b869-979a-459b-b6fb-713937539719",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### We build an empty Pinecone Index, and define the necessary LlamaIndex wrappers/abstractions so that we can start loading data into Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "238f7063-dfbc-430d-ac15-ed630c9086b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, Index, ServerlessSpec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9587703c-2a65-4cf3-b421-cae337a1f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"pinecone_api_key\"]\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5999ea0f-7c8e-4bd4-874b-8d3e3e857247",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"llamaindex-rag-fs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "158b8457-0a66-43bb-8249-43afa2baf962",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dimensions are for text-embedding-ada-002\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"euclidean\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08feb2d5-fb22-4b0b-bf90-ec6da80e6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db420df9-e566-4438-bf53-f5af43e29dca",
   "metadata": {},
   "source": [
    "### Create PineconeVectorStore\n",
    "Simple wrapper abstraction to use in LlamaIndex. Wrap in StorageContext so we can easily load in Nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceaf76a8-4921-4996-8ca6-c241b0363a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9dc6151-f851-4577-a320-d6f02cb78735",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b04a9-f0e9-4145-b4fe-8a48ed61ba45",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Build an Ingestion Pipeline from Scratch\n",
    "We show how to build an ingestion pipeline as mentioned in the introduction.\n",
    "\n",
    "Note that steps (2) and (3) can be handled via our NodeParser abstractions, which handle splitting and node creation.\n",
    "\n",
    "For the purposes of this tutorial, we show you how to create these objects manually.\n",
    "\n",
    "### 1. Load Data¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff16bc-13ba-40ec-ab03-c25c937df9f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "087f23a8-5b28-4199-9819-dd61e7ea9c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfa7106c-c680-47f2-a938-69944e74a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/llama2.pdf\"\n",
    "doc = fitz.open(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a835437-4dd5-4467-b317-1d89409b1ceb",
   "metadata": {},
   "source": [
    "### 2. Use a Text Splitter to Split Documents\n",
    "Here we import our SentenceSplitter to split document texts into smaller chunks, while preserving paragraphs/sentences as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34e684d0-b4aa-4c78-8e4f-934bebbb4866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "text_parser = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    # separator=\" \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55fc2386-4d90-46d5-9873-7880909cc6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = []\n",
    "\n",
    "doc_idxs = []\n",
    "for doc_idx, page in enumerate(doc):\n",
    "    page_text = page.get_text(\"text\")\n",
    "    cur_text_chunks = text_parser.split_text(page_text)\n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8cef39-9b51-40ea-9143-00f895029544",
   "metadata": {},
   "source": [
    "### 3. Manually Construct Nodes from Text Chunks\n",
    "We convert each chunk into a TextNode object, a low-level data abstraction in LlamaIndex that stores content but also allows defining metadata + relationships with other Nodes.\n",
    "\n",
    "We inject metadata from the document into each node.\n",
    "\n",
    "This essentially replicates logic in our SentenceSplitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf23e8fd-ade7-4472-bfdd-277df8946b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27225d41-966b-489a-a8e4-12fa48dd95ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(\n",
    "        text=text_chunk,\n",
    "    )\n",
    "    src_doc_idx = doc_idxs[idx]\n",
    "    src_page = doc[src_doc_idx]\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3dabbf7a-85f2-47fb-b0ae-ecd8cbe46df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(nodes[2].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30e287bf-9cae-4054-a4af-1ce4e908dd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗\n",
      "Louis Martin†\n",
      "Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov\n",
      "Thomas Scialom∗\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
      "source models. We provide a detailed description of our approach to fine-tuning and safety\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.\n",
      "∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "†Second author\n",
      "Contributions for all the authors can be found in Section A.1.\n",
      "arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n"
     ]
    }
   ],
   "source": [
    "# print a sample node\n",
    "print(nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa737f9-10e3-4d8e-9d3d-09c7ccdb8409",
   "metadata": {},
   "source": [
    "### [Optional] 4. Extract Metadata from each Node\n",
    "We extract metadata from each Node using our Metadata extractors.\n",
    "\n",
    "This will add more metadata to each Node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7b645-bc34-484d-8045-95ce19fdb447",
   "metadata": {},
   "source": [
    "This step — using metadata extractors like TitleExtractor and QuestionsAnsweredExtractor — is optional because it enhances the searchability and explainability of your document chunks, but is not strictly required for retrieval or embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2720b53-5e6e-43bb-84f6-28fbcf94793c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import (\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    ")\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\",api_key=os.environ['openai_api_key'])\n",
    "\n",
    "extractors = [\n",
    "    TitleExtractor(nodes=5, llm=llm),\n",
    "    QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "]\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=extractors,\n",
    ")\n",
    "nodes = await pipeline.arun(nodes=nodes, in_place=False)\n",
    "\n",
    "print(nodes[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c474e3f-797b-4eb9-b02b-177d19c3f809",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Generate Embeddings for each Node¶\n",
    "Generate document embeddings for each Node using our OpenAI embedding model (text-embedding-ada-002).\n",
    "\n",
    "Store these on the embedding property on each Node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ac78143-1bf8-43fc-8b44-fd50d15fb63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding(api_key=os.environ['openai_api_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3673f-4a03-4e77-835e-d10d014ae873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "for node in nodes:\n",
    "    time.sleep(1)  \n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b0b3f-0204-4ed2-ae36-47dbabc3e5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.embeddings.openai.base.OpenAIEmbedding._get_text_embeddings.<locals>._retryable_get_embeddings in 0.3815429388645212 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n"
     ]
    }
   ],
   "source": [
    "texts = [node.get_content(metadata_mode=\"all\") for node in nodes]\n",
    "\n",
    "# Split into chunks of 10\n",
    "for i in range(0, len(texts), 10):\n",
    "    batch = texts[i:i+10]\n",
    "    embeddings = embed_model.get_text_embedding_batch(batch)\n",
    "    for node, embedding in zip(nodes[i:i+10], embeddings):\n",
    "        node.embedding = embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c04f96-9ba8-4355-b911-eb9287b92450",
   "metadata": {},
   "source": [
    "### 6. Load Nodes into a Vector Store\n",
    "We now insert these nodes into our PineconeVectorStore.\n",
    "\n",
    "NOTE: We skip the VectorStoreIndex abstraction, which is a higher-level abstraction that handles ingestion as well. We use VectorStoreIndex in the next section to fast-track retrieval/querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7b22e-784a-4bce-bc2b-c7b582f41582",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e81105-ee43-41aa-bf39-20d412a3d869",
   "metadata": {},
   "source": [
    "## Retrieve and Query from the Vector Store\n",
    "Now that our ingestion is complete, we can retrieve/query this vector store.\n",
    "\n",
    "NOTE: We can use our high-level VectorStoreIndex abstraction here. See the next section to see how to define retrieval at a lower-level!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2acde19-68b9-4d65-857e-fb7fe33ba9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f53f555-25be-4ed6-8d3f-fae3bc382131",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a86d7-4e4e-459f-b193-db1fa9af03fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084eff52-c2fe-43db-a360-2031a8ce21c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"Can you tell me about the key concepts for safety finetuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23df2f0-b6b0-4c5c-958c-f55abbd8df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8282f2-bead-4b83-8b4e-49778140cceb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
